# @package _global_

defaults:
  - _self_
  - /dist: 8gpus
  - /model: loda
  - /data: evalmuse10k
  - /optimizer: adamW
  - /scheduler: cosineAnnealingLR
  - /loss: default
  - /log: train
  - /load: scratch

# job general configs
project_name: loda
name: loda_evalmuse10k_train_split${split_index}
run_group: loda_benchmark_evalmuse10k
working_dir: runs/${run_group}/${name}
random_seed: 3407
train_test_num: 1
num_epoch: 20
split_index: 0

# training configs
train:
  patch_num: 1
  batch_size: 8
  num_workers: 10

# test configs
test:
  patch_num: 1
  batch_size: 8
  num_workers: 10
